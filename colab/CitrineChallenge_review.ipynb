{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CitrineChallenge review.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wK-m2nm7r-RS",
        "colab_type": "text"
      },
      "source": [
        "# CitrineChallenge Review\n",
        "---\n",
        "\n",
        "\n",
        "This notebook is presented to offer a more in-depth review of the CitrineChallenge library. Specifically, to address the question, \"why do you believe this works well\".\n",
        "\n",
        "#### To Note:\n",
        "this report is interactive. Run the code cells top to bottom to make full use of the notebook.\n",
        "\n",
        "---\n",
        "\n",
        "To begin, we should make explicit what a well-working submission should accomplish. Here we have several objectives, as stated in the challenge prompt and in dialogue with the team leader:\n",
        "\n",
        "<br>\n",
        "\n",
        "- All the produced vectors satisfy all the constraints.\n",
        "\n",
        "- The vectors cover as much of the valid space as possible.\n",
        "\n",
        "- Execution should run faster than 5 minutes.\n",
        "\n",
        "- Code must be maintainable.\n",
        "\n",
        "- Code must be extensible.\n",
        "\n",
        "- Installation should be user-friendly and through Git.\n",
        "\n",
        "- Solution should be generated start-to-finish in under 10 hours.\n",
        "\n",
        "<br>\n",
        "\n",
        "The solution presented in the CitrineChallenge addresses all of these objectives to varying degrees of success, which we'll discuss below.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoG5GASYm3PN",
        "colab_type": "text"
      },
      "source": [
        "To start, making the implementation seamless for users is simply a matter of understanding how to build a setup function. It should always be an aim to build implementations which are as semaless as possible, which is what I feel we did here with the `pip + git` installation process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "no_vhl81r6o2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install git+https://github.com/1mikegrn/CitrineChallenge"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kS-Je8JnYAr",
        "colab_type": "text"
      },
      "source": [
        "This includes mandating install requirements and building in a console script entry point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeOYHUGmzO8I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import CitrineChallenge\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        "import numpy as np\n",
        "import requests\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWgwCZs8zCd8",
        "colab_type": "text"
      },
      "source": [
        "When it comes to a maintainable and extensible library system, one of the requirements I've found to be key to meeting these objectives is modularity and documentation. If a library is to be maintainable, it needs to have the ability to flex with time as new tools become available for integration, or existing tools require update. Modularity is key here. Say a team leader isn't satisfied with how new points are generated. Modularity allows a team to go into the library, find the module which is in charge of generating the data points, and tinker with it as necessary.\n",
        "\n",
        "This segues well into my commitment to thorough documentation. As Guido va Rossum has noted, *'Code is read more often than it is written'*. And to a further point, code systems become more difficult to maintain and update as it becomes more difficult for a user to figure out what a code segments objective is. This is why documentation is also key. Keying off our previous example, if a user whats to understand how the generator function works:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uU09NhZtzSa1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CitrineChallenge.src.tools.generator.generate?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYMHRX4azjXF",
        "colab_type": "text"
      },
      "source": [
        "Documentation is available. And for example when they read the documentation, they would see that generator relies on something called the 'switch' function located in src.tools.generator. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrtNnaIw0CJr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CitrineChallenge.src.tools.generator.switch?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVdhQq9H0G2M",
        "colab_type": "text"
      },
      "source": [
        "Modularity and documentation assists with the development process such that subsequent work on the library can be initialized quickly. With this, programmers can quicky find how the module works, what it takes as inputs, what it gives as outputs. And with that, they can build updates which fit around the module's paradigm.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAhgslx01UzQ",
        "colab_type": "text"
      },
      "source": [
        "When it comes to speed, the CitrineChallenge library is pretty fast. Python as an implementation is natively slow because every Python object is really a cleverly disguised C struture, which contains PyObject information on top of the actual value we assigned the object. In short, the benefit of the Python implementation is that it's flexible and dynamic, but all of this extra information which the Python interpereter interacts with ultimately slows down performance.\n",
        "\n",
        "On previous projects I've worked multiple approaches to mitigate this issue, including integrating C modules directly, building Cython-compiled modules, and using ufuncs in Numpy. Given the time constraint, we used Numpys ufuncs because their performance to development time tradeoff seemed to be the most favorable, specifically in the `switch` function, where linear interpolation is applied between the good and bad points.\n",
        "\n",
        "To demonstrate, consider the two 1D vectors below, labeled x and y."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84oGCNh_72eh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "npdata = np.random.random_sample((1000,2))\n",
        "\n",
        "np_x = npdata[:,0]\n",
        "np_y = npdata[:,1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "si2bugxsEAD-",
        "colab_type": "text"
      },
      "source": [
        "As these are the type of vectors which the library is designed to handle, we can pass them directly through the generator function and time the process:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suk_hfHD9QMD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%timeit CitrineChallenge.src.tools.generator.switch(np_x, np_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBFdKmfa_hDS",
        "colab_type": "text"
      },
      "source": [
        "This would compare to the following pure-python implementation, which we could also time:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivortTBu_pa3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pydata = [list(x) for x in npdata]\n",
        "\n",
        "py_x = [x for [x, y] in pydata]\n",
        "py_y = [y for [x, y] in pydata]\n",
        "\n",
        "py_pct = np.random.random_sample()\n",
        "\n",
        "def py_switch(py_x, py_y):\n",
        "    new_points = [(1-py_pct)*py_x[i] + py_pct*py_y[i] for i in range(len(py_x))]\n",
        "    return new_points\n",
        "\n",
        "%timeit py_switch(py_x, py_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Kl1iCmRBh0U",
        "colab_type": "text"
      },
      "source": [
        "As we can see, the pure-python implementaiton is about 2 orders of magnitude slower.\n",
        "\n",
        "We also have the added benefit of numpy's capacity to broadcast the applied mathematics over the given arrays. In python, we would need to apply the `py_switch()` function over each dimension, which would compound the computational time necessary to generate a new vector as its dimensionality is increased.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zcdO8wSGS8q",
        "colab_type": "text"
      },
      "source": [
        "Of all of the objectives listed, vector coverage is the weak spot in the CitrineChallenge implementation. One of the issues here is that the only instance where 'spread' is generated is during the initial random array generation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yf_-zxMKFnLY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/1mikegrn/CitrineChallenge/master/tests/mixture.txt'\n",
        "\n",
        "constraints = requests.get(url).text\n",
        "\n",
        "class_object = CitrineChallenge.src.constraints.Constraint(\n",
        "    requests.get(url).text.split('\\n')[:-1],\n",
        "    google=True\n",
        ")\n",
        "\n",
        "test_values = np.random.random_sample((250,2))\n",
        "report_values = np.array([class_object.get_example()])\n",
        "\n",
        "plt.scatter(test_values[:,0], test_values[:,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQVmMs0dNUk3",
        "colab_type": "text"
      },
      "source": [
        "When we initially generate the vectors, we pass them through the `clean()` function, which separates out the good points from the bad points (and also guarentees legitimate results, satisfying the objective that results must satisfy all constraints). On the below graph, blue points represent vectors which failed the check, and purple points represent vectors that passed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9TnGIztMv-v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_values, report_values = CitrineChallenge.src.tools.cleaning.clean(\n",
        "    class_object,\n",
        "    test_values=test_values,\n",
        "    report_values=report_values\n",
        ")\n",
        "\n",
        "plt.scatter(test_values[:,0], test_values[:,1], color='b')\n",
        "plt.scatter(report_values[:,0], report_values[:,1], color=(1,0,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebpSmJ0qSgw5",
        "colab_type": "text"
      },
      "source": [
        "As the program goes through and iteratively replaces the blue points with the purple points (as demonstrated below), we can see the bias that our implementation manifests - as the interpolation process is limited to the space between a good point and a bad point, this means that new data points generated will be, by definition, bound along some axis `x[i]` by the `min(x[i])` and `max(x[i])` vector components generated in the report_values array at initialization. We also have a positive feedback loop here as well, in that, as new results are found  (which are likely biased towards the inner-edge of the feasible region) the probability of the generator picking those inner points for interpolation goes up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0w96IaE4TBNc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run the cell to generate the animation. No need to tinker. \n",
        "\n",
        "# Note, once run the test_variables and report_variables are overwritten. To \n",
        "# build a new animation, first rerun the previous two cells in order, and then\n",
        "# run this cell.\n",
        "\n",
        "graph_dict = dict()\n",
        "iterator = 0\n",
        "\n",
        "while test_values.shape[0] > 0:\n",
        "\n",
        "        graph_dict[iterator] = test_values, report_values\n",
        "        iterator += 1\n",
        "\n",
        "        # clean test values according to constraints in class_object\n",
        "        test_values, report_values = CitrineChallenge.src.tools.cleaning.clean(\n",
        "            class_object=class_object,\n",
        "            test_values=test_values,\n",
        "            report_values=report_values\n",
        "        )\n",
        "\n",
        "        # calculation is done if test_values is empty\n",
        "        if test_values.shape[0] == 0:\n",
        "            break\n",
        "\n",
        "        # generate new test values by replacing rejected values\n",
        "        test_values = CitrineChallenge.src.tools.generator.generate(\n",
        "            test_values=test_values,\n",
        "            report_values=report_values\n",
        "        )\n",
        "\n",
        "graph_dict[iterator] = test_values, report_values\n",
        "\n",
        "#builds plot\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# initialize empty\n",
        "line1, = ax.plot([], [], 'o', color='b')\n",
        "line2, = ax.plot([], [], 'o', color=(1,0,1))\n",
        "\n",
        "# initialize empty\n",
        "def init():\n",
        "    line1.set_data([], [])\n",
        "    line2.set_data([], [])\n",
        "\n",
        "def animate(i):\n",
        "    line1.set_data(graph_dict[i][0][:,0], graph_dict[i][0][:,1])\n",
        "    line2.set_data(graph_dict[i][1][:,0], graph_dict[i][1][:,1])\n",
        "\n",
        "anim = animation.FuncAnimation(\n",
        "    fig, animate, init_func=init,\n",
        "    frames=len(graph_dict.keys()), interval=750\n",
        ")\n",
        "\n",
        "# don't need to show the figure\n",
        "plt.close(anim._fig)\n",
        "\n",
        "# show animation instead\n",
        "HTML(anim.to_html5_video())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajYbXdxQgddK",
        "colab_type": "text"
      },
      "source": [
        "Future versions of the library would probably want to address this. As we can see in this lower-dimensional representation, the implementation isn't *terrible* per se, but this bias compounds as the number of dimensions increases and as the feasible region becomes smaller and more irregular.\n",
        "\n",
        "For a moment I was considering adding a quick-fix to this, which was to generate something like 10x the number of points requested, and then caluclate the distances between all the points' and then return the top 10% of most-spread out points as the resultant. But during my phone conversation with Sean, he mentioned something summarized to be 'don't be too worried about the bias of an implementation' when comes to the type of programming we're doing here. And, if users wanted to get the fraction of most spread out points in the generated report_values, they could certainly do so pretty easily outside of the library. So, I ultimately decided against hard-coding that idea into the library.\n",
        "\n",
        "If I were to look into improving this bias in the future though, I would probably look into using the constraints to solve for the edges and corners of the feasible region, and then use those points as mechanisms to drag test_values towards those regions which are unfavored by the current approach. As we can see for the current example, we're constrained by the inequality:\n",
        "\n",
        "\\begin{align*}\n",
        "1.0 - x[0] - x[1] \\geq 0.0\n",
        "\\end{align*}\n",
        "\n",
        "This constraint builds a feasible region bound by the edges `x[0] + x[1] = 1`, along with `x[0] = 0` and `x[1] = 0` by the definition of the hypercube. A module which built these edge functions and then generated a set of vectors which are on those edges would help normalize the bias of the current implementation.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh9KlYa0oFU_",
        "colab_type": "text"
      },
      "source": [
        "When it's all said and done however, the current library accomplishes its objectives decently well. It's quick, guarentees legitimate results, and is built in a way which would allow for future improvements to the built-in protocols with its modularity and documentation."
      ]
    }
  ]
}